---
title: "North Beach Fort Cam"
author: "Ryan Wang"
date: "2023-07-10"
output:
    html_document:
      toc: true
      toc_float: true
      code_folding: show
---

```{r setup, include=FALSE, warning=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE,
                      warning = FALSE)

library(tidyverse)
library(tidymodels)
library(dplyr)
library(here)
library(zoo)
library(tseries)
library(stats)
library(astsa)
library(forecast)
library(car)
library(MASS)
library(finalfit) 
library(ggplot2)
library(forecast)
library(lubridate)
library(astsa)
library(dynlm)
library(knitr)
library(nlme)

nbf_hour_complete <- read.csv(here("data", "nbf_hour_complete.csv"))
```

### North Beach Fort Cam - Plotting and Analyzing the Time Series
```{r}
# take a subset for analysis  
# and split into training and testing set.

nbf_hour_ts_total_1 <- ts(nbf_hour_complete$number_of_objects[(5):(5+504+12)], frequency = 24)
nbf_hour_ts_train_1 <- ts(nbf_hour_complete$number_of_objects[(5):(5+504)], frequency = 24) 
nbf_hour_ts_test_1 <- 
  ts(nbf_hour_complete$number_of_objects[(510):(510+12)], frequency = 24, start = c(22, 1))


time_nbf_hour_ts_train <- strptime(nbf_hour_complete$time[(5):(504+5)], "%m/%d/%Y %H:%M")
tsplot(x=time_nbf_hour_ts_train, y=nbf_hour_ts_train_1, xlab = 'time', ylab = 'Count')
```

First, I plot the original data to confirm non-stationarity, where stationarity is defined as follows:

$Y_t$ is strictly stationary if $\forall k, m, t_1, ..., t_m$: $(Y_{t_1}, Y_{t_2}, . . . , Y_{t_m}) d= (Y_{t_1+k}, Y_{t_2+k}, . . . , Y_{t_m+k})$.

$Y_t$ is weakly stationary if $\forall k, s, t$: $E(Y_s) = E(Y_t)$ (no trend) and $cov(Y_t, Y_{t+k}) = cov(Y_s , Y_{s+k})$ (Constant covariance).


```{r}
# apply Box-Cox transformation to make the variance constant
bc_nbf_hour_ts_total_1<-boxcox(lm((nbf_hour_ts_total_1+0.5) ~ 1))

lambda_bc_nbf_hour_ts <- bc_nbf_hour_ts_total_1$x[which.max(bc_nbf_hour_ts_total_1$y)]
nbf_hour_ts_train <- BoxCox(nbf_hour_ts_train_1+0.5, lambda_bc_nbf_hour_ts)
nbf_hour_ts_test <- BoxCox(nbf_hour_ts_test_1+0.5, lambda_bc_nbf_hour_ts)
nbf_hour_ts_total <- BoxCox(nbf_hour_ts_total_1+0.5, lambda_bc_nbf_hour_ts)

tsplot(x=time_nbf_hour_ts_train,y=nbf_hour_ts_train, xlab = 'time', ylab = 'Transformed Count')
```

Thus, I firstly perform the Box-Cox transformation to make constant variance with $\lambda = -1.2727$. 

Mathematically, the Box-Cox transformation of the variable has the form: 

$y(\lambda) = \frac{y^{\lambda} - 1}{\lambda}$ if $\lambda  \neq 0$

$y(\lambda) = log(y)$ if $\lambda = 0$.

Thus, in this case, $y(-1.2727) = \frac{y^{-1.2727} - 1}{-1.2727}$

```{r}
acf2(nbf_hour_ts_train, max.lag = 100)

adf.test(nbf_hour_ts_train)
Box.test(nbf_hour_ts_train, lag = 24, type = c("Ljung-Box"), fitdf = 0)
```
ACF plot and PACF plot do not show slow decay of spikes or significant spikes periodically, meaning that after transformation, the time series does not have trend or seasonal component so that it is stationary.

To further confirm that the time series is stationary, I perform the ADF test. To ensure that the stationary time series is not a white noise so that the modelling is meaningful, I perform the Ljung–Box test.

ADF test:

Null Hypothesis: Series is non-stationary or series has a unit root.

Alternate Hypothesis: Series is stationary or series has no unit root.

Ljung–Box test:

Null Hypothesis: The data are independently distributed.

Alternate Hypothesis:The data are not independently distributed; they exhibit serial correlation.

The p-value of both tests are less than 0.05, and thus we can reject the null hypothesis and take that the series is not a white noise and stationary.

### North Beach Fort Cam - Model identification
```{r}
acf2(nbf_hour_ts_train, main = "North Beach Fort Cam" ) 
```
ACF shows a geometric decay over the non-seasonal lags, which could be a sign of autoregression.

PACF also shows a geometric decay over the non-seasonal lags, which could be a sign of Moving average.

Since it is a ARMA process , the exact values of p and q cannot be determined with the ACF/PACF plots.

White noise:
$Z_t$ are mean zero, constant variance and uncorrelated:
$\mu_Z(t) = E(Z_t) = 0$, $\gamma_Z(t,s) = cov(Z_t, Z_s) = 0$ if $t \neq s$; $\gamma_Z(t,t) = cov(Z_t, Z_t) = \sigma_Z^2$.


Autoregression AR(p) process:
Let $\{\epsilon_t\}\mathtt{\sim}WN(0,\sigma^2)$, then $Yt$ is an autoregressive process of order p, AR(p), if $Y_t = \phi_1Y_{t-1} + \phi_2Y_{t-2} + ... + \phi_pY_{t-p} + \epsilon_{t}$. 

Equivalently, $Y_t(1 - \phi_1B - \phi_2B^2 - ... - \phi_pB^p) = Y_t\phi(B) = \epsilon_{t}$, with characteristic polynomial $\phi(z) = (1 - \phi_1B - \phi_2B^2 - ... - \phi_pB^p)$, where $B$ denotes the backshift operator, $B^nY_t = Y_{t-n}$.


Moving average MA(q) process:
Let $\{\epsilon_t\}\mathtt{\sim}WN(0,\sigma^2)$, then $Y_t$ is a moving average process of order q, MA(q), if $Y_t = \epsilon_t + \theta_1\epsilon_{t-1} + \theta_2\epsilon_{t-2} + ...+\theta_q\epsilon_{t-q}$.

Equivalently, $\epsilon_{t}(1 + \theta_1B + \theta_2B^2 + ... + \theta_qB^q) = \epsilon_{t}\theta(B) = Y_t$, with characteristic polynomial $\theta(z) = (1 + \theta_1B + \theta_2B^2 + ... + \theta_qB^q)$.


### North Beach Fort Cam - Modelling

```{r}
# auto.arima(nbf_hour_ts_train, max.order = 2, trace=TRUE) ## Best model: ARIMA(1,0,1)(1,0,0)[24]
nbf_hour_model_auto <-arima(nbf_hour_ts_train, order=c(1,0,1),seasonal = list(order=c(1,0,0),period=24))

#### Model diagnosis - auto.arima ####
# 1. Normality test
residual_nbf_hour_model_auto <- residuals(nbf_hour_model_auto)
hist(residual_nbf_hour_model_auto,density=20,breaks=20,col="blue",xlab="",main = "Histogram of Residual",prob=TRUE)

# 2. Correlation test
acf2(residual_nbf_hour_model_auto) 
# ACF plot / PACF plot show almost no significant spike.

Box.test(residual_nbf_hour_model_auto, lag = 24, type = c("Box-Pierce"))
Box.test(residual_nbf_hour_model_auto, lag = 24, type = c("Ljung-Box"))
# Both the Box-Pierce test and the Ljung-Box test give p-vlaues greater than 0.05. I fail to reject the null
 
# Thus, I conclude that the residual of this model is a Guassian white noise.
summary(nbf_hour_model_auto)
```

I first try the model given by the auto.arima() function.
ACF and PACF plots show almost no significant spike and both the Box-Pierce test and the Ljung-Box test give p-values greater than 0.05. I fail to reject the null and conclude that the residuals are identically independently distributed. Thus, the residual of this model is a Guassian white noise.
It means that the model is already good enough since there is nothing left over that can be explained.

```{r}
nbf_hour_model_1 <-arima(nbf_hour_ts_train, order=c(3,0,1))

#### Model diagnosis - auto.arima ####
# 1. Normality test
residual_nbf_hour_model_1 <- residuals(nbf_hour_model_1)
hist(residual_nbf_hour_model_1,density=20,breaks=20,col="blue",xlab="",main = "Histogram of Residual",prob=TRUE)

# 2. Correlation test
acf2(residual_nbf_hour_model_1)
Box.test(residual_nbf_hour_model_1, lag = 24, type = c("Box-Pierce"))
Box.test(residual_nbf_hour_model_1, lag = 24, type = c("Ljung-Box"))
# Both the Box-Pierce test and the Ljung-Box test give p-vlaues greater than 0.05. I fail to reject the null 
 
# Thus, I conclude that the residual of this model is a Guassian white noise.
summary(nbf_hour_model_1)
```

Then I try the model without the seasonal component.
ACF and PACF plots show almost no significant spike and both the Box-Pierce test and the Ljung-Box test give p-values greater than 0.05. I fail to reject the null and conclude that the residuals are identically independently distributed. Thus, the residual of this model is a Guassian white noise.

```{r}
nbf_hour_model_2 <-arima(nbf_hour_ts_train, order=c(1,0,3))

#### Model diagnosis - auto.arima ####
# 1. Normality test
residual_nbf_hour_model_2 <- residuals(nbf_hour_model_2)
hist(residual_nbf_hour_model_2,density=20,breaks=20,col="blue",xlab="",main = "Histogram of Residual",prob=TRUE)

# 2. Correlation test
acf2(residual_nbf_hour_model_2) 
Box.test(residual_nbf_hour_model_2, lag = 24, type = c("Box-Pierce"))
Box.test(residual_nbf_hour_model_2, lag = 24, type = c("Ljung-Box"))
# Both the Box-Pierce test and the Ljung-Box test give p-vlaues greater than 0.05. I fail to reject the null 
 
# Thus, I conclude that the residual of this model is a Guassian white noise.
summary(nbf_hour_model_2)
```

Another model with different parameters.
ACF and PACF plots show almost no significant spike and both the Box-Pierce test and the Ljung-Box test give p-values greater than 0.05. I fail to reject the null and conclude that the residuals are identically independently distributed. Thus, the residual of this model is a Guassian white noise.

```{r}
### Summary of the created model.
result <- AIC(nbf_hour_model_auto, nbf_hour_model_1, nbf_hour_model_2) 
models <- list(nbf_hour_model_auto, nbf_hour_model_1, nbf_hour_model_2) 
result$BIC <- sapply(models, BIC) 
kable(result, digits = 2, align = "c")
```


```{r}
# Plot the fitted values with raw data
nbf_hour_fitted_1 <- nbf_hour_ts_train - residual_nbf_hour_model_1
ts.plot(nbf_hour_ts_train, main = 'model B, fitted vs actual')
points(nbf_hour_fitted_1, type = "l", col = 2, lty = 2)

nbf_hour_fitted_2 <- nbf_hour_ts_train - residual_nbf_hour_model_2
ts.plot(nbf_hour_ts_train, main = 'model C, fitted vs actual')
points(nbf_hour_fitted_2, type = "l", col = 2, lty = 2)
```

Here, I plot the fitted values of the two models with the transformed data.


### North Beach Fort Cam - Forecast
```{r}
# Plot the forecast values with actual data
pred_auto <- sarima.for(nbf_hour_ts_train, n.ahead=12, plot.all=F, p=3, d=0, q=1, P=2, D=0, Q=0, S=0)
lines(nbf_hour_ts_test, col="blue")
points(nbf_hour_ts_test, col="blue")
legend("bottomleft", pch=1, col=c("red", "blue"), legend=c("Predicted Values", "True Values"))

pred_1 <- sarima.for(nbf_hour_ts_train, n.ahead=12, plot.all=F, p=1, d=0, q=3, P=0, D=0, Q=0, S=0)
lines(nbf_hour_ts_test, col="blue")
points(nbf_hour_ts_test, col="blue")
legend("bottomleft", pch=1, col=c("red", "blue"), legend=c("Predicted Values", "True Values"))
```


```{r}
nbf_forecast_1 <- forecast::forecast(nbf_hour_model_auto,h=12,level=c(0.95))
nbf_forecast_2 <- forecast::forecast(nbf_hour_model_2,h=12,level=c(0.95))

nbf_next10 <- (lambda_bc_nbf_hour_ts*nbf_forecast_2$mean + 1)^(1/lambda_bc_nbf_hour_ts) - 0.5
# reverse the Box-Cox transformation 

plot(nbf_next10, ylim = c(0,15), col = "green", ylab = "Count", main = "Predicted vs Actual")
points(nbf_next10, col = "green")
lines(nbf_hour_ts_test_1, col = "blue")
points(nbf_hour_ts_test_1, col = "blue")
legend("topright", pch=1, col=c("blue", "green"), 
       legend=c("True Values", "Predicted Values"))
```

One explaination for skewed residual is that during the fitting process, the model is trying to minimize the sum of squared residual. So as the most of the data are around -1.5, these data pull the model's predictions towards them, resulting in a underestimation. 


